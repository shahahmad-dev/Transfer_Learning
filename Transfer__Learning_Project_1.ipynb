{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Transfer Learning with TensorFlow**\n",
        "\n",
        "## **What is Transfer Learning?**\n",
        "\n",
        "**Transfer Learning** reuses a **pre-trained model** for a new task.  \n",
        "Instead of training from scratch, we adapt knowledge learned from a **large dataset** to a **smaller dataset**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Model: `MobileNetV2`**\n",
        "\n",
        "- Pre-trained on **`ImageNet`**\n",
        "- Lightweight and efficient\n",
        "- Suitable for **image classification**\n",
        "\n",
        "---\n",
        "\n",
        "## **Dataset: `CIFAR-10`**\n",
        "\n",
        "- 60,000 images  \n",
        "- 32 × 32 pixels  \n",
        "- 10 classes  \n",
        "\n",
        "---\n",
        "\n",
        "## **Goal**\n",
        "\n",
        "- Load **`MobileNetV2`**\n",
        "- Replace the final layer\n",
        "- Fine-tune on **`CIFAR-10`**\n",
        "- Improve performance\n",
        "\n",
        "---\n",
        "\n",
        "## **Why Use Transfer Learning?**\n",
        "\n",
        "- Faster training  \n",
        "- Requires less data  \n",
        "- Better accuracy  \n",
        "\n",
        "Transfer Learning enables efficient and high-performing computer vision models."
      ],
      "metadata": {
        "id": "djZ3TZjze1bL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`Step 1: Import Libraries and Load Data`**"
      ],
      "metadata": {
        "id": "kZlqmGmwd985"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqPC16ppaM3-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load CIFAR-10 data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values and convert labels to one-hot encoding\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Augmentation**\n",
        "\n",
        "## **What is Data Augmentation?**\n",
        "\n",
        "**Data Augmentation** increases the **size** and **diversity** of training data by creating modified versions of existing data without collecting new samples.\n",
        "\n",
        "Common techniques (for images):\n",
        "\n",
        "- **Rotation**\n",
        "- **Flipping**\n",
        "- **Scaling / Zooming**\n",
        "- **Cropping**\n",
        "- **Adding Noise**\n",
        "- **Brightness Adjustment**\n",
        "\n",
        "---\n",
        "\n",
        "## **Why It Matters**\n",
        "\n",
        "- **Prevents Overfitting** – Improves generalization  \n",
        "- **Boosts Performance** – More diverse training data  \n",
        "- **Handles Imbalance** – Augments minority classes  \n",
        "\n",
        "> Always choose augmentations that make sense for your task (e.g., avoid flipping text images).\n",
        "\n",
        "---\n",
        "\n",
        "# **`Step 2: Data Preprocessing`**\n",
        "\n",
        "## **Image Size Adjustment**\n",
        "\n",
        "- **`MobileNetV2`** expects: **224 × 224**\n",
        "- **`CIFAR-10`** images are: **32 × 32**\n",
        "\n",
        "### **Required Steps**\n",
        "\n",
        "- **Resize to 224 × 224**\n",
        "- Apply **Data Augmentation**\n",
        "- **Normalize / Rescale** pixel values\n",
        "\n",
        "Proper **Augmentation + Preprocessing** ensures compatibility with **`MobileNetV2`** and improves model performance."
      ],
      "metadata": {
        "id": "cSwz2B3OeYEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center = False,\n",
        "    samplewise_center = False,\n",
        "    featurewise_std_normalization = False,\n",
        "    samplewise_std_normalization = False,\n",
        "    zca_whitening = False,\n",
        "    rotation_range = 15,\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1,\n",
        "    horizontal_flip = True ,\n",
        "    vertical_flip = True,\n",
        "    zoom_range = 0.1,\n",
        "    fill_mode = 'nearest'\n",
        ")\n",
        "\n",
        "datagen.fit(x_train)"
      ],
      "metadata": {
        "id": "zsCs3C5MeJcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`Step 3: Modify the Pre-Trained Model`**\n",
        "\n",
        "## Why Modify the Model?\n",
        "\n",
        "MobileNetV2 is pre-trained on ImageNet with 1000 classes.  \n",
        "Our task (CIFAR-10) has only 10 classes, so we must:\n",
        "\n",
        "- Remove the original top layer  \n",
        "- Add a new classifier for 10 classes  \n",
        "- Freeze the base layers (initially)\n",
        "\n",
        "---\n",
        "\n",
        "## What We Do\n",
        "\n",
        "### 1. Load MobileNetV2 Without Top Layer\n",
        "- `include_top=False`\n",
        "- Keep feature extraction layers\n",
        "- Remove the 1000-class classifier\n",
        "\n",
        "### 2. Add New Classifier\n",
        "- Global Average Pooling\n",
        "- Dense layer with 10 output units\n",
        "- Softmax activation\n",
        "\n",
        "We use fewer classes (10 instead of 1000).\n",
        "\n",
        "### 3. Freeze Base Layers\n",
        "- Pre-trained weights are not updated\n",
        "- Only the new classifier trains first\n",
        "- Later fine-tuning can be applied"
      ],
      "metadata": {
        "id": "J_0eMYxtla5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MobileNetV2 without the top layer\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "# Freeze the base_model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add custom layers on top for our task\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)  # New FC layer, random init\n",
        "predictions = Dense(10, activation='softmax')(x)  # New softmax layer\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "metadata": {
        "id": "GPIsJEiDjGI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`Step 4: Compile and Train the Model`**"
      ],
      "metadata": {
        "id": "gbkzyzW8otNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    steps_per_epoch=len(x_train) // 32, epochs=10,\n",
        "                    validation_data=(x_test, y_test), verbose=1)"
      ],
      "metadata": {
        "id": "wvN65lQVmjE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M1wBNl8rpOid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**An extra run with more epochs and Increased Batch Size**"
      ],
      "metadata": {
        "id": "z_K2CRdrrHsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(),\n",
        "loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    steps_per_epoch=len(x_train) / 64, epochs=50,\n",
        "                    validation_data=(x_test, y_test), verbose=1, callbacks=[early_stopping])\n",
        "import matplotlib.pyplot as plt\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gx9znDSgrJQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Fine-Tuning (Optional)\n",
        "\n",
        "After training the model with frozen base layers, you can improve performance by **unfreezing some layers** of MobileNetV2 and training again.\n",
        "\n",
        "Fine-tuning allows the pre-trained weights to adjust to your specific dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Fine-Tuning?\n",
        "\n",
        "- Improves accuracy\n",
        "- Adapts feature extraction to your task\n",
        "- Better performance on custom datasets\n",
        "\n",
        "---\n",
        "\n",
        "## Important Before Fine-Tuning\n",
        "\n",
        "- Unfreeze selected layers of the base model\n",
        "- Recompile the model with a lower learning rate\n",
        "\n",
        "Recompiling is necessary because model parameters are changing."
      ],
      "metadata": {
        "id": "5yob3uyPrdAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze some layers in the base model\n",
        "base_model.trainable = True\n",
        "fine_tune_at = 100  # This is the number of layers from the top to freeze\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Continue training\n",
        "history_fine = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                         steps_per_epoch=len(x_train) // 32, epochs=5,\n",
        "                         validation_data=(x_test, y_test), verbose=1)"
      ],
      "metadata": {
        "id": "EfcmY11trdua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example demonstrates the basics of applying transfer learning with TensorFlow to improve performance on a computer vision task using a smaller dataset. Fine-tuning and data augmentation are powerful techniques to increase accuracy further and adapt the pre-trained model to the new task more effectively."
      ],
      "metadata": {
        "id": "e7b4ofYernub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(histories, key='accuracy'):\n",
        "    plt.figure(figsize=(16, 4))\n",
        "\n",
        "    for name, history in histories:\n",
        "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
        "                       '--', label=name.title()+' Val')\n",
        "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
        "                 label=name.title()+' Train')\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(key.replace('_', ' ').title())\n",
        "    plt.legend()\n",
        "    plt.xlim([0, max(history.epoch)])\n",
        "\n",
        "# Plot accuracy\n",
        "plot_history([('Pre Fine-Tuning', history),\n",
        "              ('Fine-Tuning', history_fine)],\n",
        "             key='accuracy')\n",
        "\n",
        "# Plot loss\n",
        "plot_history([('Pre Fine-Tuning', history),\n",
        "              ('Fine-Tuning', history_fine)],\n",
        "             key='loss')"
      ],
      "metadata": {
        "id": "GvilYgfQro2i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}